{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65912ae4-158a-4ee6-b0f7-ec99c0dadcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from qwen3_moe_fused.fast_lora import patch_Qwen3MoeFusedSparseMoeBlock_forward\n",
    "from qwen3_moe_fused.lora import patch_lora_config\n",
    "from qwen3_moe_fused.quantize.quantizer import patch_bnb_quantizer\n",
    "from qwen3_moe_fused.modular_qwen3_moe_fused import Qwen3MoeFusedForCausalLM\n",
    "import os\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from unsloth import FastLanguageModel,FastModel\n",
    "os.environ.setdefault(\"TRITON_PRINT_AUTOTUNING\", \"1\")\n",
    "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally! \n",
    "\n",
    "# === Step 1. 打补丁 ===\n",
    "patch_bnb_quantizer()\n",
    "patch_lora_config()\n",
    "patch_Qwen3MoeFusedSparseMoeBlock_forward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f25c17f-906c-4ef2-9acf-d1263635736f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unsloth: No config file found - are you sure the `model_name` is correct?\nIf you're using a model on your local device, confirm if the folder location exists.\nIf you're using a HuggingFace online model, check if it exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/media/gpt4-pdf-chatbot-langchain/transformers-qwen3-moe-fused/moe_unsloth/checkpoint-4900\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# YOUR MODEL YOU USED FOR TRAINING\u001b[39;00m\n\u001b[1;32m      4\u001b[0m quant_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m      5\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbfloat16\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# 或 torch.float16\u001b[39;00m\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mFastLanguageModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauto_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mQwen3MoeFusedForCausalLM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m \n",
      "File \u001b[0;32m/media/gpt4-pdf-chatbot-langchain/pyenv-qwen3-moe/lib/python3.10/site-packages/unsloth/models/loader.py:216\u001b[0m, in \u001b[0;36mFastLanguageModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, qat_scheme, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     is_peft \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m model_types \u001b[38;5;241m=\u001b[39m \u001b[43mget_transformers_model_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model_types) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    218\u001b[0m     model_type \u001b[38;5;241m=\u001b[39m model_types[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/media/gpt4-pdf-chatbot-langchain/pyenv-qwen3-moe/lib/python3.10/site-packages/unsloth_zoo/hf_utils.py:107\u001b[0m, in \u001b[0;36mget_transformers_model_type\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Gets model_type from config file - can be PEFT or normal HF \"\"\"\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: No config file found - are you sure the `model_name` is correct?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre using a model on your local device, confirm if the folder location exists.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre using a HuggingFace online model, check if it exists.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    111\u001b[0m     )\n\u001b[1;32m    112\u001b[0m model_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftConfig\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unsloth: No config file found - are you sure the `model_name` is correct?\nIf you're using a model on your local device, confirm if the folder location exists.\nIf you're using a HuggingFace online model, check if it exists."
     ]
    }
   ],
   "source": [
    "model_id = \"/media/gpt4-pdf-chatbot-langchain/transformers-qwen3-moe-fused/moe_unsloth/checkpoint-4900\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=\"bfloat16\",  # 或 torch.float16\n",
    ")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_id,\n",
    "    max_seq_length = max_seq_length,\n",
    "    auto_model=Qwen3MoeFusedForCausalLM,\n",
    "    quantization_config=quant_config,\n",
    "    trust_remote_code=True,\n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b234b3a-bcd6-4f4e-a520-add7003dd5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d0fbab-e8d0-4219-861d-12d47fdecf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "messages = [\n",
    "    \n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\\nYou are designed to help with a variety of tasks, from answering questions     to providing summaries to other types of analyses.\\n\\n## Tools\\nYou have access to a wide variety of tools. You are responsible for using\\nthe tools in any sequence you deem appropriate to complete the task at hand.\\nThis may require breaking the task into subtasks and using different tools\\nto complete each subtask.\\n\\nYou have access to the following tools:\\n> Tool Name: resume_zhangmaofeng\\nTool Description: 关于张毛峰的简历信息，包括了langchain-chatchat、InterpretationoDreams、KM 平台、省检修特高压生产指挥管控系统、智能运检移动应用、福建监控系统项目eg:检索时带上详细的问题内容\\nTool Args: {\\\"type\\\": \\\"object\\\", \\\"properties\\\": {\\\"input\\\": {\\\"title\\\": \\\"Input\\\", \\\"type\\\": \\\"string\\\"}}, \\\"required\\\": [\\\"input\\\"]}\\n\\n> Tool Name: resume_liulijian\\nTool Description: 关于刘立兼的简历信息，包括了•篝火心理、雷鸟365、雷鸟365、网聚宝CRM、AP数据基盘、AP数据基盘等项目eg:检索时带上详细的问题内容\\nTool Args: {\\\"type\\\": \\\"object\\\", \\\"properties\\\": {\\\"input\\\": {\\\"title\\\": \\\"Input\\\", \\\"type\\\": \\\"string\\\"}}, \\\"required\\\": [\\\"input\\\"]}\\n\\n> Tool Name: resume_songjinke\\nTool Description: 关于宋金珂的简历信息，包括了全球 IPv4 空间内的物联网设备扫描识别和隐私安全分、开源软件生态内的跨项目依赖分析及漏洞影响追溯、已发表论文列表、IoT 设备安全等项目eg:检索时带上详细的问题内容\\nTool Args: {\\\"type\\\": \\\"object\\\", \\\"properties\\\": {\\\"input\\\": {\\\"title\\\": \\\"Input\\\", \\\"type\\\": \\\"string\\\"}}, \\\"required\\\": [\\\"input\\\"]}\\n\\n\\n## Output Format\\nTo answer the question, please use the following format.\\n\\n```\\nThought: I need to use a tool to help me answer the question.\\nAction: tool name (one of resume_zhangmaofeng, resume_liulijian, resume_songjinke) if using a tool.\\nAction Input: the input to the tool, in a JSON format representing the kwargs (e.g. {\\\"input\\\": \\\"hello world\\\", \\\"num_beams\\\": 5})\\n```\\n\\nPlease ALWAYS start with a Thought.\\n\\nPlease use a valid JSON format for the Action Input. Do NOT do this {'input': 'hello world', 'num_beams': 5}.\\n\\nIf this format is used, the user will respond in the following format:\\n\\n```\\nObservation: tool response\\n```\\n\\nYou should keep repeating the above format until you have enough information\\nto answer the question without using any more tools. At that point, you MUST respond\\nin the one of the following two formats:\\n\\n```\\nThought: I can answer without using any more tools.\\nAnswer: [your answer here]\\n```\\n\\n```\\nThought: I cannot answer the question with the provided tools.\\nAnswer: Sorry, I cannot answer your query.\\n```\\n\\n## Current Conversation\\nBelow is the current conversation consisting of interleaving human and assistant messages.\\n\\n\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"请问候选人的全名是什么？\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Thought: 我需要使用工具来获取候选人的全名。\\nAction: resume_zhangmaofeng\\nAction Input: {'input': '请问候选人的全名是什么？'}\"\n",
    "            }\n",
    "        \n",
    " \n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(\n",
    "    input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "    use_cache = True, temperature = 1.5, min_p = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb18005-d38d-4b09-8e51-c53ce0152e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyenv-qwen3-moe] *",
   "language": "python",
   "name": "conda-env-pyenv-qwen3-moe-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
